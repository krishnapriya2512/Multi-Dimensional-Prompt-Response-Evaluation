# Multi-Dimensional Evaluation and Application of Generative AI: A Study of Prompt Effectiveness, Ethical Risks, and Real-World Use Cases

This repository contains a collection of experiments and implementations around **Prompt Engineering and Evaluation of LLMs, GPT vs Traditional Models,Bias & Toxicity Detection, Text-to-Image Generation, and Resume Evaluation**.  

The project explores different aspects of **Generative AI** using **Gemini API, GPT models, Transformers, Stable Diffusion, and Traditional NLP methods**, alongside various **evaluation metrics and visualizations**.  

---

## üìë Table of Contents  
1. [Prompt Engineering & Evaluation of LLM Response](#a-prompt-engineering-and-evaluation-of-llm-response)  
2. [GPT vs Traditional Model Performance](#b-gpt-vs-traditional-model-performance-for-tasks)  
3. [Bias & Toxicity Detection](#c-bias-and-toxicity-detection-in-the-text-using-llm)  
4. [Text-to-Image Generation & Relevance Evaluation](#d-text-to-image-generation-using-llm--relevance-evaluation)  
5. [Resume Evaluation & Feedback](#e-resume-evaluation-and-feedback-using-llm)  
6. [Technologies Used](#technologies-used)  

---

## A. Prompt Engineering and Evaluation of LLM Response  

- **Dataset**: 100 prompts across 10 categories: *Factual, Creative Writing, Instructional, Philosophical, Casual, Analytical, Professional, Personal Growth, Technical, Open-ended*.  
- Prompts were applied to the **Gemini model**, and the resulting responses were evaluated.

### üìù Evaluation Metrics  
- Response Length & Word Count  
- Sentiment Polarity & Subjectivity  
- Lexical Diversity  
- Grammar Errors & Grammar Score
- Clartiy and Coherence
- Prompt and response relevance

### üìä Visualizations  
- Prompt and Response length by category  
- Prompt and Response Polarity & Subjectivity by category  
- Prompt and response Lexical diversity by category  
- Grammer Score and Grammatical error for prompt and Response
- Clarity and Coherence plots for Prompt and Response by category
- Correlation heatmap between numerical variables
- Prompt and Response relevance by Category

‚úÖ Automated pipeline for evaluation  

### Folder Structure
**Folder**: Prompt Engineering and Evaluation of LLM Response                                          
**Subfolders**: 
- *Prompt_engineering.ipynb*: This is the main python file. It contains responses generated by Gemini API, Visualisations and insights for the responses generated.
- *prompt_engineering.csv*: File contains 100 prompts used for prompt engineering.
- *PE_Evalmetrics.csv*: File contains Evaluated metrics of responses generated by Gemini API for Prompt Engineering.
- *PE_Responses.csv*: File contains responses generated by Gemini API for Prompt Engineering.
- *Prompt Engineering_promptEval.ipynb*: File contains evaluation metrics for 100 prompts created for Prompt Engineeing.
- *Prompt_Evalmetrics.csv*: File contains evaluated metrics of prompts created for Prompt Engineeing.

---

## B. GPT Vs Traditional Model Performance for Tasks  

### 1. Sentiment Analysis  
- **Dataset:** Amazon Polarity  
- **Traditional Approach:** Text Preprocessing ‚Üí Tfidf Vectorization ‚Üí Logistic Regression ‚Üí Hyperparameter tuning ‚Üí Prediction & Evaluation(Classification Report)
- **Transformer Approach:** HuggingFace pipeline for distilbert-base-uncased-finetuned-sst-2-english ‚Üí Training ‚Üí Prediction & Evaluation (Classification Report)
- **Result:** Traditional Model outperformed Transformer for this task.  

### 2. Text Summarization  
- **Dataset:** CNN/DailyMail  
- **Traditional Approach:** Text Extraction ‚Üí Tokenisation using nltk ‚Üí Textrank summariser (sumy) ‚Üí Evaluation of generated summary using Rouge score
- **Transformer Approach:** Hugging Face Tokenization ‚Üí Summary generation -> Evaluation of generated text
- **Evaluation Metrics:**  
  - Cosine & Semantic Similarity  
  - Lexical Diversity  
  - Compression Ratio  
  - Sentiment shift (Article vs Summary)  
  - Repetition & Readability Score  

- **Visualizations:** Comparative chart of all metrics  

- **Results:**  Traditional models are more reliable for factual accuracy and staying close to the original text. On the other hand, GPT models create more varied, natural-sounding, and easy-to-read content, which makes them better for things like chatbots and other applications that interact directly with users.

### Folder Structure
**Folder**: GPT vs Tradtional models performance for tasks                                             
**Subfolders**:
- *GPT_vs_Traditional_models.ipynb*: This is the main Python code file. This file contains the comparison of Traditional and Transformer models for tasks **Sentiment Analysis and Text summarisation**.
- *CR_Sentiment_GPT.csv*: File contains classification report of sentiment analysis of transformer-based model.
- *CR_Sentiment_traditional.csv*: File contains classification report of sentiment analysis of logistic regression model
- *Sentiment_prediction_gpt.csv*: File contains true and predicted values of transformer-based model.
- *Sentiment_prediction_tra.csv*: File contains true and predicted values of logistic regression model.
- *Text_summarise_metrics.csv*: File contains text summarisation evaluation metric of both traditional as well as transformer-based model. 

---

## C. Bias and Toxicity Detection in the Text using LLM  

### 1. Bias Detection  
- **100 biased prompts** across categories: Gender, Religion, Ethnicity, Socioeconomic, Sexual Orientation, Neutral  
- Used Gemini API to Classify type of Bias  
- **Evaluation:** Classification Report + Confusion Matrix  

### 2. Toxicity Detection  
- **Models Used:**  
  - Perspective API  
  - Detoxify  
  - Gemini API (custom toxicity scoring)  

- **Evaluation Metrics:** Toxicity, Severe Toxicity, Insult, Profanity, Threat, Identity Attack, Number of Misclassifications. 
- **Visualizations:**  
  - Confusion matrices & classification reports  
  - Radar chart for overall comparison
- **Results:** Gemini API performed well compared to the other two models, with fewer misclassifications

### Folder Structure
**Folder**: Bias and Toxicity detection in text                                           
**Subfolders**:   
- *Bias_Toxicity_detection.ipynb*: This is the main Python Code file. It contains the responses generated by models for Bias and Toxicity, their evaluations, visualisations and insights.
- *Bias_prompts.csv*: File contains 100 prompts with different Bias prompts.
- *Bias_responses.csv*: File contains the responses Generated by Gemini API for respective prompts.
- *Toxicity_prompt.csv*: File contains 100 prompts with different Toxicity prompts.
- *Toxicity score using Gemini API.ipynb*: This Python file is used generate toxicity sores for gemini API generated responses.
- *Toxic_response_d.csv*: File contains toxicity responses generated by detoxify.
- *Toxic_response_g.csv*: File contains toxicity responses generated by Gemini API.
- *Toxicity_response_p.csv*: File contains toxicity responses generated by Perspective API.
- *Toxicity_scores_gemini*: Files contains the toxicity scores generated by Gemini API. 

---

## D. Text-to-Image Generation using LLM & Relevance Evaluation  
- **Dataset**: 100 Prompts across 5 categories: *Nature, Cityscapes & structures, People, Commercial Products, Artistic*.
  - Prompts were applied to Gemini Model as well as Stable Diffusion and further evaluated by Metrics as well as humans.

- **Models Used**:
    - Gemini API: gemini-2.0-flash-preview-image-generation
    - Stable Diffusion: stable-diffusion-v1-5

- **Evaluation:**
    - Clip Score
    - Blip Score
    - Inception Score
    - Human Evaluation

**Result:** Gemini outperformed Stable Diffusion in terms of relevance and clarity 

### Folder Structure
**Folder**: Bias and Toxicity detection in text                                           
**Subfolders**:     
- *Text _to_image_generation_modified.ipynb*: This the main Python code file for image generation using gemini and evalution of scores for both gemini as well as Stable diffusion.
- *Stable_diffusion_generation.ipynb*: File contains code for image generation using stable diffusion.
- *Image_Evalmetrics.csv*: File contains the evaluated metrics for generated images using both models.
- *inception_score_per_category.csv*: File contains inceptions scores per image category generated for Gemini generated images.
- *inception_score_per_category_SD.csv*: File contains inceptions scores per image category generated for Stable Diffusion generated images.

---

## E. Resume Evaluation and Feedback using LLM  

- Extract resume text (PDF parsing)  
- Resume and Job Description match percentage
- Matched Skills with Job Description
- Detailed Evaluation of Resume which Includes tone, clarity and coherence  
- **Personalized feedback** on resume improvement using Gemini API.  

---

## üõ†Ô∏è Technologies Used  

- **LLMs & APIs:** Gemini API, HuggingFace Transformers(BERT, BART), Stable Diffusion, Detoxify, Perspective API
- **Traditional NLP:** Scikit-learn, Sumy, NLTK
- **Visualization:** Matplotlib, Seaborn, Radar charts, Pairplots, Heatmaps, Barplots & Grouped Bar plots, Confusion Matrix, Plotly   
- **Deployment:** Streamlit Cloud

