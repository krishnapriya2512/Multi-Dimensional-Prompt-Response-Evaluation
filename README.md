# Multi-Dimensional Evaluation and Application of Generative AI: A Study of Prompt Effectiveness, Ethical Risks, and Real-World Use Cases

This repository contains a collection of experiments and implementations around **Prompt Engineering and Evaluation of LLMs, GPT vs Traditional Models,Bias & Toxicity Detection, Text-to-Image Generation, and Resume Evaluation**.  

The project explores different aspects of **Generative AI** using **Gemini API, GPT models, Transformers, Stable Diffusion, and Traditional NLP methods**, alongside various **evaluation metrics and visualizations**.  

---

## üìë Table of Contents  
1. [Prompt Engineering & Evaluation of LLM Response](#a-prompt-engineering-and-evaluation-of-llm-response)  
2. [GPT vs Traditional Model Performance](#b-gpt-vs-traditional-model-performance-for-tasks)  
3. [Bias & Toxicity Detection](#c-bias-and-toxicity-detection-in-the-text-using-llm)  
4. [Text-to-Image Generation & Relevance Evaluation](#d-text-to-image-generation-using-llm--relevance-evaluation)  
5. [Resume Evaluation & Feedback](#e-resume-evaluation-and-feedback-using-llm)  
6. [Technologies Used](#technologies-used)  

---

## A. Prompt Engineering and Evaluation of LLM Response  

- **Dataset**: 100 prompts across 10 categories: *Factual, Creative Writing, Instructional, Philosophical, Casual, Analytical, Professional, Personal Growth, Technical, Open-ended*.  
- Prompts were applied to the **Gemini model**, and the resulting responses were evaluated.

### üìù Evaluation Metrics  
- Response Length & Word Count  
- Sentiment Polarity & Subjectivity  
- Lexical Diversity  
- Grammar Errors & Grammar Score
- Clartiy and Coherence
- Prompt and response relevance

### üìä Visualizations  
- Prompt and Response length by category  
- Prompt and Response Polarity & Subjectivity by category  
- Prompt and response Lexical diversity by category  
- Grammer Score and Grammatical error for prompt and Response
- Clarity and Coherence plots for Prompt and Response by category
- Correlation heatmap between numerical variables
- Prompt and Response relevance by Category

‚úÖ Automated pipeline for evaluation  

#### Folder Structure
**Folder**: Prompt Engineering and Evaluation of LLM Response
**Subfolders**: 
- *Prompt_engineering.ipynb*: This is the main file. It contains responses generated by Gemini API, Visualisations and insights for the responses generated. 
- *PE_Evalmetrics.csv*: File contains Evaluated metrics of responses generated by Gemini API for Prompt Engineering.
- *PE_Responses.csv*: File contains responses generated by Gemini API for Prompt Engineering.
- *Prompt Engineering_promptEval.ipynb*: File contains evaluation metrics for 100 prompts created for Prompt Engineeing.
- *Prompt_Evalmetrics.csv*: File contains evaluated metrics of prompts created for Prompt Engineeing.

---

## B. GPT Vs Traditional Model Performance for Tasks  

### 1. Sentiment Analysis  
- **Dataset:** Amazon Polarity  
- **Traditional Approach:** Text Preprocessing ‚Üí Tfidf Vectorization ‚Üí Logistic Regression ‚Üí Hyperparameter tuning ‚Üí Prediction & Evaluation(Classification Report)
- **Transformer Approach:** HuggingFace pipeline for distilbert-base-uncased-finetuned-sst-2-english ‚Üí Training ‚Üí Prediction & Evaluation (Classification Report)
- **Result:** Traditional Model outperformed Transformer for this task.  

### 2. Text Summarization  
- **Dataset:** CNN/DailyMail  
- **Traditional Approach:** Text Extraction ‚Üí Tokenisation using nltk ‚Üí Textrank summariser (sumy) ‚Üí Evaluation of generated summary using Rouge score
- **Transformer Approach:** Hugging Face Tokenization ‚Üí Summary generation -> Evaluation of generated text
- **Evaluation Metrics:**  
  - Cosine & Semantic Similarity  
  - Lexical Diversity  
  - Compression Ratio  
  - Sentiment shift (Article vs Summary)  
  - Repetition & Readability Score  

**Visualizations:** Comparative chart of all metrics  

---

## C. Bias and Toxicity Detection in the Text using LLM  

### 1. Bias Detection  
- **100 biased prompts** across categories: Gender, Religion, Ethnicity, Socioeconomic, Sexual Orientation, Neutral  
- Used Gemini API to Classify type of Bias  
- **Evaluation:** Classification Report + Confusion Matrix  

### 2. Toxicity Detection  
- **Models Used:**  
  - Perspective API  
  - Detoxify  
  - Gemini API (custom toxicity scoring)  

- **Evaluation Metrics:** Toxicity, Severe Toxicity, Insult, Profanity, Threat, Identity Attack, Number of Misclassifications. 
- **Visualizations:**  
  - Confusion matrices & classification reports  
  - Radar chart for overall comparison
- **Results:** Gemini API performed well compared to the other two models, with fewer misclassifications

---

## D. Text-to-Image Generation using LLM & Relevance Evaluation  
- **Dataset**: 100 Prompts across 5 categories: *Nature, Cityscapes & structures, People, Commercial Products, Artistic*.
  - Prompts were applied to Gemini Model as well as Stable Diffusion and further evaluated by Metrics as well as humans.

- **Models Used**:
    - Gemini API: gemini-2.0-flash-preview-image-generation
    - Stable Diffusion: stable-diffusion-v1-5

- **Evaluation:**
    - Clip Score
    - Blip Score
    - Inception Score
    - Human Evaluation

**Result:** Gemini outperformed Stable Diffusion in terms of relevance and clarity 

---

## E. Resume Evaluation and Feedback using LLM  

- Extract resume text (PDF parsing)  
- Resume and Job Description match percentage
- Matched Skills with Job Description
- Detailed Evaluation of Resume which Includes tone, clarity and coherence  
- **Personalized feedback** on resume improvement using Gemini API.  

---

## üõ†Ô∏è Technologies Used  

- **LLMs & APIs:** Gemini API, HuggingFace Transformers(BERT, BART), Stable Diffusion, Detoxify, Perspective API
- **Traditional NLP:** Scikit-learn, Sumy, NLTK
- **Visualization:** Matplotlib, Seaborn, Radar charts, Pairplots, Heatmaps, Barplots & Grouped Bar plots, Confusion Matrix, Plotly   
- **Deployment:** Streamlit Cloud

